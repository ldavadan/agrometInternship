```{r include_packages_ch5, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
library(devtools)
devtools::install_github("haozhu233/kableExtra")
library(kableExtra)
load("./data/env_report.RData")
```


<!-- And then, I aggregated, by calculating the mean, all the hourly performance measures to choose the method that globally performs the best. For each desired hourly dataset, I applied the choosen method to build a model to make spatial predictions. The predictions and their uncertainty have been vizualized using maps. -->

# Results and discussion {#results}

## Benchmark

### Methodology

A LOOCV strategy of the multiple linear regression learning algorithm has been applied to each set of hourly records. The aim is to assess combinations of explanatory variables provides, on average, the best spatial predictions. This is achieved by comparing and ranking the various combinations of explanatory variables with respect to their performance measures (MAE and RMSE) averaged on the whole set of hourly records. Hence we actually end up with a so-called mlr benchmark experiment that is run on each set of hourly records.

The initial objective was to run these benchmarks on 5 years of data to establish which combination of explanatory variables provides, on average, the best predictions. But at this moment, solar irradiance data from EUMETSAT was not available for the whole period. These benchmarks were therefore performed on 23089 sets of hourly records from 2015-11-11 00:00:00 to 2018-06-30 00:00:00 (the period for which the solar data is already available).

&nbsp;

The learners were defined with filter methods implemented in `mlr`. These filter methods are applied to a statistical algorithm (multiple linear regression) to choose explanatory variables using conditions. The different conditions used to filter explanatory variables are the following :

- for each hourly dataset, linear correlation with temperature is computed for every explanatory variable. Other filter methods are available (chi-squared, anova...) but linear correlation seems to be the most relevant one for a regression problem.
- then, explanatory variables are kept according to conditions : variables with the best hourly linear correlation with temperature (the number of variables to keep is specified) or all the variables which have a linear correlation greater than a specific value from 0 to 1.
- I can also choose explanatory variables that I want to build models regardless of their linear correlation

All these filter methods where applied to **Multiple Linear Regression** learner in the case of my internship.

&nbsp;

The Table \@ref(tab:explvar) shows the different combinations that were created using filter methods.

&nbsp;

Because a benchmark is performed on each set of hourly records, computation of the linear correlation is also performed on an hourly basis scale and mlr choose automatically the variables using the filter methods. Thus, it must be stressed out that the selected combination of explanatory variables may differ from one hour to another.

&nbsp;

Performing the benchmarks on the 23089 sets of hourly records took about 30 hours, i.e. 3 hours per method. Computations are very long and results are very large. Each method represents more than 1 Gigabyte of data.

```{r explvar, echo=FALSE}
explvar = data.frame(Statistical_Method = c("Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression", "Multiple Linear Regression"),
                     ID = c("lm.Long.Lat", "lm.Long.Lat.Elev", "lm.SolIrr+1bestVar", "lm.SolIrr+2bestsVar", "lm.SolIrr+3bestsVar", "lm.2bestsVar", "lm.3bestsVar", "lm.4bestsVar", "lm.Vars.r>0,5", "lm.Vars.r>0,3"),
                     Explanatory_Variables = c("Longitude & Latitude", "Longitude & Latitude & Elevation", "Solar Irradiance & best variable based on an hourly linear correlation computation", "Solar Irradiance & 2 best variables based on an hourly linear correlation computation", "Solar Irradiance & 3 best variables based on an hourly linear correlation computation", "2 best variables based on linear correlation computation for every hour", "3 best variables based on linear correlation computation for every hour", "4 best variables based on linear correlation computation for every hour", "Variables with a linear correlation greater than 0.5", "Variables with a linear correlation greater than 0.3"))
colnames(explvar) <- c("Statistical Method", "ID", "Explanatory variables")
knitr::kable(explvar, caption = "Combination of explanatory variables used") %>%
  kable_styling(latex_options = "scale_down") %>%
  row_spec(0, bold = TRUE)
```

### Comparison of methods

Once the 23089 benchmarks are performed, we can compare their outputs averaged on the whole period. This comparison is based on the error of measures. In our case, RMSE and MAE were computed because they both express average model prediction error in units of the variable of interest. Both metrics can range from 0 to $\infty$ and are indifferent to the direction of errors. They are negatively-oriented scores, which means lower values are better.

Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is more useful because large errors are particularly undesirable in the project. (Chai, 2014)

&nbsp;

The figure \@ref(fig:meanerror) shows the averaged RMSE and MAE for the 10 methods implemented. The two metrics both return the same ranking of the methods and they both show methods which stand out from the other ones.

The method only using coordinates, i.e. longitude and latitude, to build models has a large error, this combination is not relevant to spatially predict temperature in Wallonia. The method using explanatory variables with a linear correlation with temperature greater than 0.3 has an error larger than the other methods too, this filter method is not selective enough to return valid spatial predictions. A few methods have a similar error. In particular, that is the case when 4 variables are chosen to build models.

&nbsp;

Among the 3 methods that, on average, perform the best, the one that builds the spatial prediction models with longitude, latitude and altitude as explanatory variables, provides the lowest errors. The two other methods are based on the hourly computation of the linear correlations with temperature, with or without solar irradiance as mandatory variable and keeping the 2 other best variables have a similar error. However, in spite of their larger error, they can be interesting because the equation is dynamic throughout hours and, in this way, the models are adapted to the evaluated hour.

```{r meanerror, echo=FALSE, fig.cap="Errors (RMSE and MAE) of methods", out.width="100%", fig.align='center'}
include_graphics(path = "figure/meanerror.png")
```

Errors are between 0.72 and 0.91 for MAE and between 0.93 and 1.20 for RMSE. These errors should be near zero. Both of MAE and RMSE are expressed in degrees such as temperature. An error of 1 degree is relatively important and has to be taken into consideration.

&nbsp;

Performances of methods can be compared computing their rank for each hour. The Figure \@ref(fig:barchart) compares the three best methods :

- the 2 variables with the best hourly linear correlation with temperature
- longitude, latitude and elevation
- solar irradiance and the 2 variables with the hourly best linear correlation with temperature

This barchart corroborates the precedent graph. The method based on coordinates and elevation is widely better than the two others which are more similar but with a relevant difference. The ranks $1.5$ and $2.5$ correspond to cases where two methods have exactly the same error for a same task.

```{r barchart, echo=FALSE, fig.cap="Comparison of methods by rank", out.width="100%", fig.align='center'}
include_graphics(path = "figure/barchart.png")
```

`mlr` benchmark allows to extract the equations of the computed models. This is useful if we want to build spatialized temperature maps. The Table \@ref(tab:models) shows an example of the structure of the equations in the case where explanatory variables can be different from one hour to another (lm.2bestsVar). The table also shows what are the best variables and display the related error.

```{r models, echo=FALSE, fig.cap="Models with their equations"}
knitr::kable(models.sample, caption = "Models with their equations") %>%
  kable_styling(latex_options = "scale_down") %>%
  row_spec(0, bold = TRUE)
```

### Visualization

Spatial predictions built upon these models can be shown on maps. For that purpose, functions building maps have been made with ggplot2 library from R for static maps and leaflet library for interactive maps.

Models built from physical stations data are applied to the 1 km² grid cells. Then, the spatialized temperature is mapped with a color palette similar to the one of RMI. Class breaks are based on quantiles of temperature values. Standard error is computed for each cell and it is shown on the map with a white layer which has different levels of transparency according to the error. A large standard error is related to an opacity and vice-versa. This transparency trick to display the attached standard error was inspired from the article of Hengl (2011) which explains the uncertainty of the prediction at each interpolation point.

The Figure \@ref(fig:map) shows an example of output that I can produce for one hour based on the method where explanatory variables are Solar irradiance and the 2 variables with the best linear correlation with temperature. To build this map, some objects are needed : an object containing data (temperature and standard error) for the grid and a spatial vector object containing boundaries of Wallonia, but also the name of the variable to display. Then, some conditions can be chosen, like the display of the layer containing error, the display of the legend for error, the way to build the legend and its classes. Some arguments enable to customize the map with titles and comments. The function is thus reusable for other usages. For example, this function has also been used to build maps of spatialized hydric deficit in Wallonia during this year summer drought.

The choice I made was to use quantiles to make class breaks because that is more relevant than homogeneous breaks. Indeed, the last one has an unequal count of 1 km² cells per grid whereas groups made with quantile classification have the same quantity. A reference about the difference between these classifications is available in _Appendix C_

The figure shows 2 maps. The map on the left shows the spatialized temperature. The map on the right shows the spatialized temperature combined with the standard error (uncertainty) of the prediction. It is possible to see larger errors in the Ardennes, in the Meuse river valley near to Liège and in the east of Hainaut province.

```{r map, echo=FALSE, fig.cap="Example of an output for 2018-05-02 14:00:00 (left : without standard error ; right : with standard error)", out.width="100%"}
include_graphics("figure/maperror.png")
```

This figure shows the output based on a model depending on the method where explanatory variables are Solar irradiance and the 2 variables with the best linear correlation with temperature, the equation of the model is the following one :

\[
T = 15.59716 + -0.00629 \times Elevation + -0.00197 \times Herbaceous + 0.00206 \times SolarIrradiance
\]

The _Appendix B_ shows exemplative maps made with all methods for one hour. These maps show differences in the relevance of each model. For example, the model depending on longitude and latitude is very simplistic compared to the others. The other models are more similar but show that some of them are more reliable because the standard error is smaller. The better model is the second one on the first row. It is corresponding to the the model depending on longitude, latitude and elevation. For this hour, the equation of the best model is :
\[
T = -9.375042 + -0.010431 \times Elevation + 1.02e-05 \times Longitude + 1.59e-05 \times Latitude
\]

Longitude and latitude are expressed in meters because the CRS used is Belgian Lambert 2008. That is why their coefficients are about 1$e$-05. For information, values of the coordinates are about 6$e$06.

## Discussion

As a reminder, the objective of the project is to provide spatialized weather data with the lowest uncertainty and the lowest prediction error in the agricultural areas of Wallonia. These predictions will feed decision support tools to monitor crop diseases like potato late blight and to operate in fields at the right times.

```{r rmi, echo=FALSE, fig.cap="Precipitations, temperatures and insolation, annual values", out.width="100%", fig.align='center'}
include_graphics(path = "figure/rmi_climate_data.png")
```

Results have some limits that have to be discussed. The first limit is the period used to build models. Indeed, models were built with data from 2 and a half years. This period could be too short to be relevant. Moreover, this period is not necessarily representative of a mean period. According to RMI, 2015, 2017 and 2018 are hot and dry years compared to normal year, 2016 were a wet year. This is shown on the Figure \@ref(fig:rmi).

&nbsp;

Even if visualization of outputs is not the main objective of the project, the choice I made to compute class breaks with quantiles can be discussed. In the case of temperature, that seems relevant but other methods exist.

&nbsp;

Models were built with some explanatory variables but they may be insufficient. Adding new variables like temperature predictions from RMI could improve models. Only 27 stations are used to build these models, adding new variables like next hour temperature predictions from RMI could improve models too. However, weather stations from different networks have differences in their measures, checking interoperability is very important for that and making corrections is essential.

&nbsp;

Multiple linear regression is the only statistical method used to build models, but going forward, other methods will be compared like ANN and different kriging methods. The major constraint will be time computation which is relatively long. There is a lot of possible combinations of explanatory variables to compare, choices must be done because of the time computation. These choices sometimes can be subjective and not based on scientific literature.

&nbsp;

Beyond these limits, other points can be discussed. Solar irradiance data are provided by EUMETSAT and by PAMESEB stations. These data have to be compared because both sources are used. Data from stations are used to build models, data from EUMETSAT for spatialization. To do this comparison, I have searched for the nearest point of record from EUMETSAT to each PAMESEB station. Then, I compared each couple on the period where I realised the benchmark. This comparison shows a correlation around 0.95. As a consequence, there is nothing wrong with it.

&nbsp;

The ranking of the methods show that the method using the hourly 2 best variables provides a smaller error than the same method adding solar irradiance in every task. That is interesting because intuitively, adding an explanatory variable should reduce the error. I noticed that solar irradiance provides a large error for a few tasks, that might explain these results.

&nbsp;

With the aim to provide data for agronomic utilisation, there is an interest to compare mean error observed in Wallonia and this error in agricultural areas to be sure of the accuracy of predictions. That has not be done yet.

&nbsp;

Rather than choosing the best spatial prediction method on average to perform the hourly spatialization, we could also decide to choose, for each hourly spatialization, the best method according to the benchmark even if this method could be different for each hour. This would not be a problem as long as the methodology is transparent.

