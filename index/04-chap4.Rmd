```{r include_packages_3, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("bookdown", repos = "http://cran.rstudio.com")
if(!require(thesisdown)){
  library(devtools)
  devtools::install_github("ismayc/thesisdown")
  }
library(thesisdown)
flights <- read.csv("data/flights.csv")
```


# Results and discussion {#results}

## Benchmark

### Methodology

To realize the benchmark, data from 2015-11-11 00:00:00 to 2018-06-30 00:00:00 were used. The objective is to run a benchmark on 5 years of data but at this moment, solar irradiance data from EUMETSAT were not available before this date. Here, the dataset has 23089 hours.

The learners were defined with filter methods, i.e. the same statistical method was applied to different combinations of explanatory variables.

The Table \@ref(tab:explvar) shows the different combinations used and compared.

Every computation is done for each hour. As a consequence, the combination of explanatory variables is not unique for each hour but depends on a condition checked every time.

Performances were measured with a Leave-One-Out cross-validation resampling strategy.

The benchmark took 30 hours, i.e. 3 hours per method. Computations are very long and results are very large. Each method represents 1 Gigabyte of data.

------------------------------------------------------------------------------------------------------------------
  Statistical Method   ID                                   Explanatory Variables
---------------------  ------------------------  -----------------------------------------------------------------
Multiple Linear         _lm.Long.Lat_                 Longitude & Latitude
Regression

Multiple Linear         _lm.Long.Lat.Elev_            Longitude & Latitude & Elevation
Regression

Multiple Linear         _lm.SolIrr+1bestVar_        Solar Irradiance & best variable 
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.SolIrr+2bestsVar_       Solar Irradiance & 2 bests variables
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.SolIrr+3bestsVar_       Solar Irradiance & 3 bests variables 
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.2bestsVar_                2 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.3bestsVar_               3 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.4bestsVar_                4 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.Vars.r>0,5_               Variables with a linear correlation greater than 0.5
Regression

Multiple Linear         _lm.Vars.r>0,3_               Variables with a linear correlation greater than 0.3
Regression
------------------------------------------------------------------------------------------------------------------
Table : (\#tab:explvar) Combinations of explanatory variables used

### Comparison of methods

Once benchmark results are available, comparison of methods is possible. This comparison is based on the error of measures. In our case, RMSE and MAE were computed.

MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It is the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.
\[
MAE = \frac{1}{n} \sum_{i=1}^{n}{ \lvert y_{j} - \widehat{y_{j}} \rvert}
\]

RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.

\[
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n}{( y_{j} - \widehat{y_{j}} )^2}}
\]

They both express average model prediction error in units of the variable of interest. Both metrics can range from 0 to $\infty$ and are indifferent to the direction of errors. They are negatively-oriented scores, which means lower values are better.

Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is more useful because large errors are particularly undesirable in the project.

From the 10 methods compared, MAE and RMSE are computed and compared. The results are shown in the Figure \@ref(fig:meanerror). In this case, they both have the same behaviour. Multiple linear regression using coordinates to build models has a large error, the model is too simplified. Multiple linear regression using explanatory variables whose their linear correlations with temperature is greater than 0.3 has an error larger than the other methods too, this filter method is too flexible to return a valid model. A few methods have a similar error. In particular, that is the case when too many variables are chosen to build models.

On the three best methods, one is better than the others. That is the model built from an equation using longitude, latitude and altitude as explanatory variables. Tests realized on two months of data have already shown that altitude is a powerful explanatory variable. The two other methods are based on the computation of the linear correlations with temperature, with or without solar irradiance as mandatory variable have a similar error. However, they are more interesting because the equation is dynamic throughout hours and, in this way, the model is adapted to the evaluated hour.

```{r meanerror, echo=FALSE, fig.cap="Figure 3 : Errors (RMSE and MAE) of methods"}

```

Errors are between 0.72 and 0.91 for MAE and between 0.93 and 1.20 for RMSE. These errors should be near zero. Both of MAE and RMSE are expressed in degrees such as temperature. An error of 1 degree is relatively important and to be taken into consideration.

### Vizualisation

These models can be observed on maps. For that purpose, functions building maps have been made with ggplot2 library from R for static maps and leaflet library for interactive maps.

Models built from physical stations data are applied to the 1 km² grid cells. Then, the temperature is mapped with a color palette similar to the one of RMI. Class breaks are based on quantiles of temperature values. Standard error is computed for each cell and it is shown on the map with a white layer which has different levels of transparency according to the error. A large standard error is related to an opacity and vice-versa.

The Figure \@ref(fig:map) shows an output for one hour based on the method ::todo::.

```{r map, echo=FALSE, fig.cap="Figure 4 : Example of an output for one hour (date)"}

```

These maps show whether the models are relevant. The _Annex 2_ shows maps made with all methods for one hour. ::todo:: interpretation

::todo:: add example equations models tab

## Discussion

As a reminder, the objective of the project is to provide weather predictions. These predictions will feed decision support tools to monitor crop diseases like potato late blight and to operate in fields at the right times.

Results have some limits that have to be discussed. The first limit is the period used to build models. Indeed, models were built with data from 2 and a half years. This period could be too short to be relevant. Moreover, this period is not necessarily realistic of a mean period. According to RMI, 2015, 2017 and 2018 are hot and dry years compared to normal year, 2016 were a wet year and 2018. This is shown on the Figure \@ref(fig:rmi).

```{r rmi, echo=FALSE, fig.cap="Precipitations, temperatures and insolation, annual values", out.height="300px", out.width="400px", fig.align='center'}
include_graphics(path = "figure/rmi_climate_data.png")
```

Models were built with some explanatory variables but they may be insufficient. Adding new variables like temperature predictions from RMI could improve models. Only 27 stations are used to build these models, adding RMI stations network could be a solution to improve models too. However, weather stations from different networks have differences in their measures, checking interoperability is very important for that and making corrections is essential.

Multiple linear regression is the only statistical method used to build models, but going forward, other methods will be compared like ANN and different kriging methods. The major constraint will be time computation which is relatively long.

Beyond these limits, other points can be discussed. For example, solar irradiance data are provided by EUMETSAT and by PAMESEB stations. These data have to be compared because both sources are used. Data from stations are used to build models, data from EUMETSAT for spatialization. The comparison shows a correlation around 0.95. As a consequence, there is nothing wrong with it.

With the aim to provide data for agronomic utilisation, there is an interest to compare mean error observed in Wallonia and this error in agricultural areas to be sure of the accuracy of predictions. ::todo::

Assuming that no other models will be better than those presented, there will be a question of transparency for the project. Indeed, as a public project within the scope of public agencies, the work that is done has to be transparent. In that way, models will be clearly presented. If one static model is chosen for predictions, this will be clear and transparent. But if the chosen method is a dynamic one, using computation every hour, the method used for each hour could be different, and that will have to be mentioned.





