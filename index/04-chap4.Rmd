```{r include_packages_ch4, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
library(devtools)
library(tidyverse)
devtools::install_github("ismayc/thesisdown")
library(thesisdown)
library(mlr)
library(sf)
library(purrr)
library(sp)
library(raster)
devtools::install_github("pokyah/geoTools")
library(geoTools)
devtools::install_github("pokyah/agrometAPI")
library(agrometAPI)
devtools::install_github("haozhu233/kableExtra")
library(kableExtra)

source("~/Documents/code/agrometeoR-mlr/R/file_management.R")
source_files_recursively.fun("~/Documents/code/agrometeoR-mlr/R")
load("./data/env_report.RData")
```


# Results and discussion {#results}

## Benchmark

### Methodology

To realize the benchmark, data from 2015-11-11 00:00:00 to 2018-06-30 00:00:00 were used. The objective is to run a benchmark on 5 years of data but at this moment, solar irradiance data from EUMETSAT were not available before this date. Here, the dataset has 23089 hours.

The learners were defined with filter methods, i.e. the same statistical method was applied to different combinations of explanatory variables.

The Table \@ref(tab:explvar) shows the different combinations used and compared.

Every computation is done for each hour. As a consequence, the combination of explanatory variables is not unique for each hour but depends on a condition checked every time.

Performances were measured with a Leave-One-Out cross-validation resampling strategy.

The benchmark took about 30 hours, i.e. 3 hours per method. Computations are very long and results are very large. Each method represents more than 1 Gigabyte of data.

------------------------------------------------------------------------------------------------------------------
  Statistical Method   ID                                   Explanatory Variables
---------------------  ------------------------  -----------------------------------------------------------------
Multiple Linear         _lm.Long.Lat_                 Longitude & Latitude
Regression

Multiple Linear         _lm.Long.Lat.Elev_            Longitude & Latitude & Elevation
Regression

Multiple Linear         _lm.SolIrr+1bestVar_          Solar Irradiance & best variable 
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.SolIrr+2bestsVar_         Solar Irradiance & 2 bests variables
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.SolIrr+3bestsVar_         Solar Irradiance & 3 bests variables 
Regression                                          based on an hourly linear correlation 
                                                    computation 
                                                     
Multiple Linear         _lm.2bestsVar_                2 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.3bestsVar_               3 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.4bestsVar_                4 bests variables based on linear correlation 
Regression                                          computation for every hour
                                                     
Multiple Linear         _lm.Vars.r>0,5_               Variables with a linear correlation greater than 0.5
Regression

Multiple Linear         _lm.Vars.r>0,3_               Variables with a linear correlation greater than 0.3
Regression
------------------------------------------------------------------------------------------------------------------
Table : (\#tab:explvar) Combinations of explanatory variables used

### Comparison of methods

Once benchmark results are available, comparison of methods is possible. This comparison is based on the error of measures. In our case, RMSE and MAE were computed.

MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It is the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.
\[
MAE = \frac{1}{n} \sum_{i=1}^{n}{ \lvert y_{j} - \widehat{y_{j}} \rvert}
\]

RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It is the square root of the average of squared differences between prediction and actual observation.

\[
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n}{( y_{j} - \widehat{y_{j}} )^2}}
\]

They both express average model prediction error in units of the variable of interest. Both metrics can range from 0 to $\infty$ and are indifferent to the direction of errors. They are negatively-oriented scores, which means lower values are better.

Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is more useful because large errors are particularly undesirable in the project.

From the 10 methods compared, MAE and RMSE are computed and compared. The results are shown in the Figure \@ref(fig:meanerror). In this case, they both have the same behaviour. Multiple linear regression using coordinates to build models has a large error, the model is too simplified. Multiple linear regression using explanatory variables whose their linear correlations with temperature is greater than 0.3 has an error larger than the other methods too, this filter method is too flexible to return a valid model. A few methods have a similar error. In particular, that is the case when too many variables are chosen to build models.

On the three best methods, one is better than the others. That is the model built from an equation using longitude, latitude and altitude as explanatory variables. Tests realized on two months of data have already shown that altitude is a powerful explanatory variable. The two other methods are based on the computation of the linear correlations with temperature, with or without solar irradiance as mandatory variable have a similar error. However, they are more interesting because the equation is dynamic throughout hours and, in this way, the model is adapted to the evaluated hour.

```{r meanerror, echo=FALSE, fig.cap="Errors (RMSE and MAE) of methods", out.width="100%", fig.align='center'}
include_graphics(path = "figure/meanerror.png")
```

Errors are between 0.72 and 0.91 for MAE and between 0.93 and 1.20 for RMSE. These errors should be near zero. Both of MAE and RMSE are expressed in degrees such as temperature. An error of 1 degree is relatively important and has to be taken into consideration.

Performances of methods can be compared computing their rank for each hour. The Figure \@ref(fig:barchart) compares five of the bests methods :

- the 2 variables with the best linear correlation with temperature
- longitude, latitude and elevation
- solar irradiance and the 2 variables with the best linear correlation with temperature

This barchart corroborates the precedent graph. The method based on coordinates and elevation is widely better than the two others which are more similar but with a relevant difference.

```{r barchart, echo=FALSE, fig.cap="Comparison of methods by rank", out.width="100%", fig.align='center'}
include_graphics(path = "figure/barchart.png")
```

For each hour, the equation of the model is computed. They can be extracted from the benchmark. The Table \@ref(tab:models) shows the structure of the equations in the case where explanatory variables can be different from one hour to another (lm.2bestsVar). The table also shows what are the bests variables and display the related error.

```{r models, echo=FALSE, fig.cap="Models with their equations"}
knitr::kable(models.sample, caption = "Models with their equations") %>%
  kable_styling(latex_options = "scale_down") %>%
  row_spec(0, bold = TRUE)
```

### Vizualisation

These models can be observed on maps. For that purpose, functions building maps have been made with ggplot2 library from R for static maps and leaflet library for interactive maps.

Models built from physical stations data are applied to the 1 kmÂ² grid cells. Then, the temperature is mapped with a color palette similar to the one of RMI. Class breaks are based on quantiles of temperature values. Standard error is computed for each cell and it is shown on the map with a white layer which has different levels of transparency according to the error. A large standard error is related to an opacity and vice-versa.

The Figure \@ref(fig:map) shows an output for one hour based on the method ::todo::. To build this map, some objects are needed : an object containing data (temperature and standard error) for the grid and a spatial vector object containing boundaries of Wallonia, but also the name of the variable to display. Then, some conditions can be chosen, like the display of the layer containing error, the display of the legend for error, the way to build the legend and its classes. Some arguments enable to customize the map with titles and comments. The function is thus reusable for other usages. For example, the function builds maps spatializing hydric deficit in Wallonia.

This figure shows the output based on a model depending on ::todo::, the equation is the following one :

\[

\]

```{r map, echo=FALSE, fig.cap="Example of an output for one hour (date)"}

```

These maps show whether the models are relevant. The _Appendix B_ shows maps made with all methods for one hour. These maps show differences in the relevance of each model. For example, the model depending on longitude and latitude is very simplistic compared to the others. The other models are more similar but show that some of them are more reliable because the error is smaller. The better model is the second one on the first row. It is corresponding to the the model depending on longitude, latitude and elevation. For this hour, the model is the following :
\[
T = -9.375042 + -0.010431 \times Elevation + 1.02e-05 \times Longitude + 1.59e-05 \times Latitude
\]

Longitude and latitude are expressed in meters because the CRS used is Belgian Lambert 2008. That is why their coefficients are about 1e-05. For information, values of the coordinates are about 6e06.

::todo:: interpretation

::todo:: add example equations models tab

## Discussion

As a reminder, the objective of the project is to provide weather predictions. These predictions will feed decision support tools to monitor crop diseases like potato late blight and to operate in fields at the right times.

Results have some limits that have to be discussed. The first limit is the period used to build models. Indeed, models were built with data from 2 and a half years. This period could be too short to be relevant. Moreover, this period is not necessarily representative of a mean period. According to RMI, 2015, 2017 and 2018 are hot and dry years compared to normal year, 2016 were a wet year. This is shown on the Figure \@ref(fig:rmi).

```{r rmi, echo=FALSE, fig.cap="Precipitations, temperatures and insolation, annual values", out.height="300px", out.width="400px", fig.align='center'}
include_graphics(path = "figure/rmi_climate_data.png")
```

Models were built with some explanatory variables but they may be insufficient. Adding new variables like temperature predictions from RMI could improve models. Only 27 stations are used to build these models, adding RMI stations network could be a solution to improve models too. However, weather stations from different networks have differences in their measures, checking interoperability is very important for that and making corrections is essential.

Multiple linear regression is the only statistical method used to build models, but going forward, other methods will be compared like ANN and different kriging methods. The major constraint will be time computation which is relatively long. There is a lot of possible combinations of explanatory variables to compare, choices must be done because of the time computation. These choices sometimes can be subjective and not based on scientific literature.

Beyond these limits, other points can be discussed. For example, solar irradiance data are provided by EUMETSAT and by PAMESEB stations. These data have to be compared because both sources are used. Data from stations are used to build models, data from EUMETSAT for spatialization. The comparison shows a correlation around 0.95. As a consequence, there is nothing wrong with it.

With the aim to provide data for agronomic utilisation, there is an interest to compare mean error observed in Wallonia and this error in agricultural areas to be sure of the accuracy of predictions. ::todo::

Assuming that no other models will be better than those presented, there will be a question of transparency for the project. Indeed, as a public project within the scope of public agencies, the work that is done has to be transparent. In that way, models will be clearly presented. If one static model is chosen for predictions, this will be clear and transparent. But if the chosen method is a dynamic one, using computation every hour, the method used for each hour could be different, and that will have to be mentioned.
